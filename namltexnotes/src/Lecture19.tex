

\newtheorem{theorem}{Theorem}


\section{Newton's Method}

We begin by reviewing Newton's method in one dimension for two contexts: finding the root of a function and finding the minimum of a function. We then extend the method to higher dimensions. Recall that a function $f$ is $C^2$ if $f$ is twice differentiable and $f''$ is continuous.


\subsubsection*{Root-Finding Problem}

Let $\alpha$ be a zero of $f$. For $f \in C^2(\mathbb{R})$:

\begin{equation}
    f(\alpha) = 0 = f(x) + (\alpha - x)f'(\xi)
\end{equation}

where $\xi$ is between $x$ and $\alpha$. This leads to the iterative formula:

\begin{equation}
    x^{(k+1)} = x^{(k)} - \frac{f(x^{(k)})}{f'(x^{(k)})} \quad \forall k \geq 0
\end{equation}

given an initial guess $x^{(0)}$.

\subsubsection*{Minimization Problem}

For $f \in C^2(\mathbb{R})$, we use the second-order Taylor approximation:

\begin{equation}
    f(x^{(k)} + \epsilon) = f(x^{(k)}) + \epsilon f'(x^{(k)}) + \frac{\epsilon^2}{2}f''(x^{(k)}) \tag{$\star$}
\end{equation}

To minimize this approximation, we set its derivative to zero:

\begin{equation}
    \frac{d}{d\epsilon}(\star) = f'(x^{(k)}) + \epsilon f''(x^{(k)}) = 0 \implies \epsilon = -\frac{f'(x^{(k)})}{f''(x^{(k)})}
\end{equation}

This yields the iterative formula:

\begin{equation}
    x^{(k+1)} = x^{(k)} - \frac{f'(x^{(k)})}{f''(x^{(k)})} \quad \forall k \geq 0 \tag{$\star\star$}
\end{equation}

given an initial guess $x^{(0)}$.

\subsubsection*{Multi-Dimensional Newton's Method}

The goal of Newton's method in higher dimensions is to find:

\begin{equation}
    \min_{\underline{w}} J(\underline{w})
\end{equation}

A necessary condition for $\underline{w}^*$ to be a minimum is that $\nabla J(\underline{w}^*) = \underline{0}$. 

The multi-dimensional Newton's method is a generalization of ($\star\star$):

\begin{equation}
    \underline{w}^{(k+1)} = \underline{w}^{(k)} - \mathbf{H} (\underline{w}^{(k)})^{-1}\nabla J(\underline{w}^{(k)}) \quad \forall k \geq 0
\end{equation}

where $\mathbf{H} (\underline{w}^{(k)})$ is the Hessian matrix of $J$ at $\underline{w}^{(k)}$. Note that $\mathbf{H} (\underline{w}^{(k)})$ must be non-singular for this method to work.

We define the search direction:

\begin{equation}
    \underline{d}(\underline{w}^{(k)}) = - \mathbf{H} (\underline{w}^{(k)})^{-1}\nabla J(\underline{w}^{(k)})
\end{equation}

\textbf{Note:} If $J$ is convex, then $\mathbf{H} (\underline{w}^{(k)})$ is positive definite, implying:

\begin{equation}
    \underline{d}(\underline{w}^{(k)})^\top \nabla J(\underline{w}^{(k)}) = - \nabla J(\underline{w}^{(k)})^\top \mathbf{H} (\underline{w}^{(k)})^{-1}\nabla J(\underline{w}^{(k)}) < 0
\end{equation}

This means that $\underline{d}(\underline{w}^{(k)})$ is a descent direction. If $J$ is not convex, Newton's method may converge to local minimizers or saddle points.

\begin{theorem}
Let $J \in C^2(\mathbb{R}^n)$, the Hessian be L-Lipschitz continuous, and $\underline{w}^*$ be a stationary point. If $\mathbf{H} (\underline{w}^*)$ is nonsingular, then there exists a neighborhood of $\underline{w}^*$, denoted $\mathcal{B}_{\rho}(\underline{w}^*)$ (a ball of radius $\rho$ centered at $\underline{w}^*$), such that for any $\underline{w}^{(0)} \in \mathcal{B}_{\rho}(\underline{w}^*)$, the sequence generated by Newton's method converges quadratically to $\underline{w}^*$, and each iteration $\underline{w}^{(k)}$ remains in $\mathcal{B}_{\rho}(\underline{w}^*)$.
\end{theorem}

\textbf{Note:} $\rho$ depends on the Hessian, and $\underline{w}^*$ is typically unknown. $\mathcal{B}_{\rho}(\underline{w}^*)$ is called the domain of quadratic attraction of $\underline{w}^*$.\\

\textbf{Computational Consideration:} Computing the Hessian $\mathbf{H}$ and solving $\mathbf{H}\underline{d} = \nabla J$ is computationally expensive, especially for high-dimensional problems.\\

\subsection{Quasi-Newton method}
The idea is to approximate the Hessian with a matrix $\mathbf{B}_k = \mathbf{H}(\underline{w}^{(k)})$ which is positive definite and invertible. We then set 
\[
    \underline{w}^{(k+1)} = \underline{w}^{(k)} + \alpha_k \underline{d}_k 
\]
Where $\underline{d}_k$ is the direction of descent and $\alpha_k$ is the step size. In particular, we set $\underline{d}_k = - \mathbf{B}_k^{-1}\nabla J(\underline{w}^{(k)})$.\\

The algorithm is the following:
\begin{enumerate}
    \item Choose $\underline{w}^{(0)} \in \mathbb{R}^n$, $\mathbf{B}_0 \in \mathbb{R}^{n \times n}$ nonsingular (often $\mathbf{B}_0 = I$), $\epsilon > 0$ and $k = 0$
    \item If $\|\nabla J(\underline{w}^{(k)})\| < \epsilon$ stop
    \item Compute $\underline{d}_k = - \mathbf{B}_k^{-1}\nabla J(\underline{w}^{(k)})$
    \item Perform a line-search for minimizing $\phi(\alpha) = J(\underline{w}^{(k)} + \alpha\underline{d}_k)$: find $\alpha_k$ that satisfy the Wolfe conditions and set $\underline{w}^{(k+1)} = \underline{w}^{(k)} + \alpha \underline{d}_k$ 
    \item Compute $\mathbf{B}_{k+1}$ (according to some rule)
    \item $k \to k+1$, goto step 2
\end{enumerate}
$\alpha_k$ satisfy the Wolfe conditions if, for a given direction $\underline{d}_k$:
\begin{enumerate}
    \item $J(\underline{w}^{(k)} + \alpha_k\underline{d}_k) \leq J(\underline{w}^{(k)}) + c_1 \alpha_k \underline{d}_k^\intercal \nabla J(\underline{w}^{(k)}) $ (Armijo condition)
    \item $- \underline{d}_k^\intercal \nabla J(\underline{w}^{(k)} + \alpha_k\underline{d}_k) \leq -c_2 \underline{d}_k^\intercal \nabla J(\underline{w}^{(k)})$ (curvature condition)
\end{enumerate}
And $0 < c_1 < c_2 < 1$.\\

\textbf{Remember: } the first condition is a sufficient decrease condition on values of $\alpha_k$, the second condition is a curvature condition (reduction of the slope). They give an upper and lower bound on $\alpha_k$.\\

\textbf{Remember: } if $\underline{d}_k = - \mathbf{B}_k^{-1}\nabla J(\underline{w}^{(k)})$ and $\textbf{B}_k$ is positive definite then if $\textbf{B}_k$ is properly updated then also $\textbf{B}_{k+1}$ is positive definite. \\


Let's now list all the desired properties of $\mathbf{B}_k$:
\begin{enumerate}[i]
    \item $\textbf{B}_k$ should be non-singular
    \item $\textbf{B}_k$ should be such that $\underline{d}_k$ is a descent direction
    \item $\textbf{B}_k$ should be symmetric (as $\textbf{H}$)
    \item $\textbf{B}_{k+1}$ should be computable by using $\nabla J(\underline{w}^{(k+1)}), \nabla J(\underline{w}^{(k)}), \dots, \nabla J(\underline{w}^{(0)}), \underline{d}_k, \alpha_k$.
    \item $\textbf{B}_{k+1}$ should be "close" to $\textbf{B}_{k}$ (so $\textbf{B}_{k}$ can converge to $\textbf{H}(\underline{w}^*)$ and $\underline{d}_k$ is allowed to become the Newton step asymptotically)
    \item $\textbf{B}_{k}$ should be such that the computational cost per iteration is at most $O(n^2)$ compared to $O(n^3)$ for the Newton method. 
\end{enumerate}

\textbf{Remember}: $i$ and $iii$ are satisfied if $\textbf{B}_k$ is symmetric positive definite. For $ii$ we have:
\[
    \underline{d}_k^\intercal \nabla J(\underline{w}^{(k)}) = -\nabla J(\underline{w}^{(k)})^\intercal \mathbf{B}_k^{-1} \nabla J(\underline{w}^{(k)}) < 0 \iff \textbf{B}_k \text{ is positive definite}    
\]
This avoids also QN method to get attracted to any point but a local minimizer.\\

\textbf{Remember: } $iv$ can be obtained if the "secant condition" is satisfied, i.e.:
\[
    \mathbf{B}_{k+1} \underline{\delta}_k = \underline{\gamma}_k   
\]
Where
\[
    \underline{\gamma}_k = \nabla J(\underline{w}^{(k+1)}) - \nabla J(\underline{w}^{(k)}) \hspace{0.2cm} \text{ and } \hspace{0.2cm} \underline{\delta}_k = \alpha_k \underline{d}_k
\]

\textbf{Remember: } to quantify the distance between $\textbf{B}_{k+1}$ and $\textbf{B}_{k}$ we can use a norm or by keeping the rank of $\textbf{B}_{k+1} - \textbf{B}_{k}$ as low as possible.\\

\subsection{Symmetric rank 1 updates}
$iii$ and $iv$ can be satisfied by requiring:
\[
    \mathbf{B}_{k+1} = \mathbf{B}_k + \underline{u}\underline{u}^\intercal \hspace{0.3cm} \text{rank 1 update}   
\]
By enforcing the secant conditions. In particular:
\[
    \mathbf{B}_{k+1} = \mathbf{B}_k + \underline{u}\underline{u}^\intercal \implies \mathbf{B}_{k+1} \underline{\delta}_k = \mathbf{B}_k \underline{\delta}_k + (\underline{u}\underline{u}^\intercal) \underline{\delta}_k \implies (\underline{u}^\intercal \underline{\delta}_k)\underline{u} = \underline{\gamma}_k - \mathbf{B}_k \underline{\delta}_k
\]
Transposing and multiplying by $\underline{\delta}_k$:
\[
    (\underline{u}^\intercal \underline{\delta}_k)^2 = (\underline{\gamma}_k - \mathbf{B}_k\underline{\delta}_k)^\intercal \underline{\delta}_k \implies \underline{u} = \frac{\underline{\gamma}_k - \mathbf{B}_k\underline{\delta}_k}{\underline{u}^\intercal \underline{\delta}_k}    
\]
Hence
\[
    \mathbf{B}_{k+1} = \mathbf{B}_k + \frac{(\underline{\gamma}_k - \mathbf{B}_k\underline{\delta}_k)(\underline{\gamma}_k - \mathbf{B}_k\underline{\delta}_k)^\intercal}{(\underline{\gamma}_k - \mathbf{B}_k\underline{\delta}_k)^\intercal \underline{\delta}_k}    \hspace{0.2cm} \text{SR1}
\]
Since $\underline{\gamma}_k = \nabla J (\underline{w}^{(k+1)}) - \nabla J (\underline{w}^{(k)})$ and $\underline{\delta}_k = \alpha_k \underline{d}_k$ the update requires only already known quantities.\\

SR1 is 
\begin{itemize}
    \item positive: easy to compute
    \item negative: $\mathbf{B}_k$ not always positive
    \item negative: $\underline{d}_k$ might not always be defined or be a descent direction
    \item negative: $(\underline{\gamma}_k - \mathbf{B}_k\underline{\delta}_k)^\intercal\underline{\delta}_k$ can be close to zero so we can have large updates
\end{itemize}

\textbf{Remember: } When $\underline{d}_k$ is known the computations of $\alpha_k, w^{(k+1)},\nabla J(\underline{w}^{(k+1)}), \underline{\gamma}_k$ and $\underline{d}_k$ are very cheap. The computation of the outer product requires computing $n^2$ entries, adding 2 $n\times n$ matrices requires $n^2$ additions so the cost is $O(n^2)$. But we have also to solve the linear system, if $d_k$ is unknown:
\[
    \mathbf{B}_k\underline{d}_k = - \nabla J(\underline{w}^{(k)})\hspace{0.5cm} O(n^3)\text{ ops required}    
\]

\textbf{Theorem (Sherman-Morrison-Woodbury): } if $B \in \mathbb{R}^{n\times n}$ and $U,V \in \mathbb{R}^{n \times p}$ then 
\[
    (B + UV^\intercal)^{-1} = B^{-1} - B^{-1}U(I + V^\intercal B^{-1}U)^{-1}V^\intercal B^{-1}    
\]

\textbf{Remember: } if we knew $A_k = \mathbf{B}_k^{-1}$, applying SMW to $B_{+} = \mathbf{B}_{k+1}, B = \mathbf{B}_k, U = \underline{u} = (\underline{\gamma}_k - \mathbf{B}_k\underline{\delta}_k)$ and $V = U^\intercal \hspace{0.1cm}(p=1)$ we have:
\[
    A_{k+1} = (B_+)^{-1} = B^{-1} - B^{-1}\underline{u}(1+\underline{u}^\intercal B^{-1}\underline{u})^{-1}\underline{u}^\intercal B^{-1} = A_k + \dfrac{(\underline{\delta}_k - A_k\underline{\gamma}_k)(\underline{\delta}_k - A_k\underline{\gamma}_k)^\intercal}{(\underline{\delta}_k - A_k\underline{\gamma}_k)^\intercal \underline{\gamma}_k}
\] 
This implies that $A_{k+1}$ is a rank 1 update of $A_k$. Since we have assumed that $A_k$ is known, computing $\underline{d}_k = -A_k \nabla J(\underline{w}^{(k)})$ is $O(n^2)$. Hence computing $\underline{\delta}_k$ and $\underline{\gamma}_k$ is $O(n^2)$ plus the outer product which is $O(n^2)$. $A_{k+1}$ can be computed from $A_k$ in $O(n^2)$. \\

\textbf{Remember: } if we start with $\mathbf{B}_0$ with known inverse (for example $\mathbf{B}_0 = I$) then we never need to build $\mathbf{B}_k$. \\

\textbf{Remember: } SR1 converges superlinearly in a neighborhood of a local minimizer \footnote[2]{A converging sequence $x^{(k)} \to x^*$ has a convergence rate $r\geq 1$ if $\exists \rho > 0$ and $k_0$ such that 
\[
    \|x^{(k+1)} - x^*\| \leq \rho \|x^{(k)} - x^*\|^r \hspace{0.2cm} \forall k \geq k_0 \hspace{1cm} \lim_{k \to \infty} \frac{\|x^{(k+1)} - x^*\|}{\|x^{(k)} - x^*\|} = 0 \hspace{0.2cm} \text{superlinear convergence}
\]
for $r = 1$ then $\rho < 1.$
}.

\textbf{Drawbacks of SR1:} $\mathbf{B}_k$ is not guaranteed to be positive definite, $\underline{d}_k$ is not guaranteed to be a descent direction.\\


\subsection{BFGS}
The Broyden-Fletcher-Goldfarb-Shanno algorithm satisfies all properties, from $i$ to $vi$. The idea is:
\[
    \mathbf{B}_{k+1} = \mathbf{B}_k + \underline{u}\underline{u}^\intercal + \underline{v}\underline{v}^\intercal \hspace{0.3cm} \underline{u},\underline{v}\text{ linearly independent} 
\] 
So it is a rank 2 update. Important to remember that SR1 was the best rank 1 update while this method, BFGS, is the best rank 2 update.
\[
    \mathbf{B}_{k+1} = \mathbf{B}_k - \frac{\mathbf{B}_k\underline{\gamma}_k\underline{\gamma}_k^\intercal \mathbf{B}_k}{\underline{\delta}_k^\intercal \mathbf{B}_k \underline{\delta}_k} + \frac{\underline{\gamma}_k\underline{\gamma}_k^\intercal}{\underline{\gamma}_k^\intercal \underline{\delta}_k} \hspace{0.3cm} \text{ where } \begin{cases}
        \underline{\gamma}_k = \nabla J(\underline{w}^{(k+1)}) - \nabla J(\underline{w}^{(k)})\\
        \underline{\delta}_k = \underline{w}^{(k+1)} - \underline{w}^{(k)} = \alpha_k \underline{d}_k
    \end{cases}
\]

\textbf{Remember: }\\
When the search direction \(\underline{d}_k\) is known, the computational cost of the BFGS update consists of two main parts:

\begin{itemize}
  \item The update itself: \(O(n^2)\) operations, where \(n\) is the dimension of the optimization problem.
  \item Solving the linear system \(\mathbf{B}_k \underline{d}_k = -\nabla J(\underline{w}^{(k)})\) to find the next search direction. The solution of this linear system can be efficiently computed using the Sherman-Morrison-Woodbury (SMW) formula, which takes advantage of the low-rank structure of the update. 
\end{itemize}


A reduction in complexity is obtained using Cholensky factorization. If $\mathbf{B}_k$ is positive definite and we know $\mathbf{B}_k = \mathbf{L}_k \mathbf{L}_k^\intercal$ where $\mathbf{L}_k$ is lower triangular, then:
\[
    \mathbf{B}_k \underline{d}_k = - \nabla J(\underline{w}^{(k)}) \iff \begin{cases}
        \mathbf{L}_k \underline{g}_k = - \nabla J(\underline{w}^{(k)})\\
        \mathbf{L}_k^\intercal \underline{d}_k = \underline{g}_k
    \end{cases}
\]

If the Cholensky decomposition of $\mathbf{B}_k$ is known then computing $\underline{d}_k$ is $O(n^2)$ and so the cost of BFGS per iteration is $O(n^2)$ as well.\\

\textbf{Idea: } 
\begin{itemize}
    \item find an update rule $L_k \leftarrow L_{k+1}$ (minimizing $d(L_{k+1}, L_k)$)
    \item compute $\mathbf{B}_{k+1} = \mathbf{L}_{k+1}\mathbf{L}_{k+1}^\intercal$ (positive definite)
    \item $L_{k+1}$ is chosen such that $\mathbf{B}_{k+1}$ satisfies the secant conditions (property $iv$)
\end{itemize}

\textbf{The actual BFGS algorithm:}
\begin{enumerate}
    \item Choose $\underline{w}^{(0)}$ and $L_0$ with positive diagonal entries ($L_0 = I$); choose $\epsilon > 0$
    \item If $\|\nabla J(\underline{w}^{(k)})\| < \epsilon$ stop
    \item Otherwise solve $\mathbf{L}_k \underline{g}_k = - \nabla J(\underline{w}^{(k)})$ and $\mathbf{L}_k^\intercal \underline{d}_k = \underline{g}_k$
    \item Perform a line search to find $\alpha_k > 0$ such that $J(\underline{w}^{(k)} +  \alpha\underline{d}_k) < J(\underline{w}^{(k)})$ and such that the Wolfe conditions are satisfied
    \item Set $\underline{\delta}_k = \alpha_k \underline{d}_k$, $\underline{w}^{(k+1)} = \underline{w}^{(k)} + \underline{\delta}_k$. Compute $\underline{\gamma}_k = \nabla J(\underline{w}^{(k+1)}) - \nabla J(\underline{w}^{(k)})$ and $\beta_k = \pm \sqrt{\dfrac{\underline{\gamma}_k^\intercal\underline{\delta}_k}{\underline{\delta}_k^\intercal \mathbf{B}_k \underline{\delta}_k}}$
    \item Compute $J_{k+1}^\intercal = L_k^\intercal + \dfrac{L_k^\intercal \underline{\delta}_k (\underline{\gamma}_k - \beta_k \mathbf{B}_k \underline{\delta}_k)^\intercal}{\beta_k \underline{\delta}_k^\intercal \mathbf{B}_k \underline{\delta}_k}$ 
    \item and then compute the QR factorization $J_{k+1}^\intercal = Q_{k+1}R_{k+1}$ 
    \item Set $L_{k+1} = R_{k+1}^\intercal$ and return to step 2
\end{enumerate}

\textbf{Remember:} the cost of BFGS per iteration is $O(n^2)$ and the convergence is superlinear. If BFGS is used for strictly convex quadratic functions in conjunction with exact line search then $\mathbf{B}_k$ is the exact costant Hessian after $n$ iterations.\\

\subsubsection*{Summary of convergence rates and costs}
\begin{center}
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Method} & \textbf{Cost per iteration} & \textbf{Convergence rate}\\
        \hline
        Steepest descent & $O(n\cdot c(J))$ & linear\\
        \hline
        Quasi-Newton & $O(n^2 + n\cdot c(J))$ & superlinear\\
        \hline
        Newton & $O(n^3 + n^2\cdot c(J))$ & quadratic\\
        \hline
    \end{tabular}
\end{center}
 