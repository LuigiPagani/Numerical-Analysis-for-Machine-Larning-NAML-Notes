\section{Automatic Differentiation}

In this section, we will explore different methods for computing the derivative of a function:

\begin{center}
\begin{tabular}{|| m{14em} | m{12em} | m{20em} ||} 
 \hline
 Method & PROS & CONS \\ [0.5ex] 
 \hline\hline
 Manual computation & Exact, useful for proofs & Prone to error, time-consuming for complex functions\\ 
 \hline
 Numerical differentiation & Easy to program & Issues with floating point precision*, computationally expensive\\
 \hline
 Symbolic differentiation & Exact, good for proofs & Expression swelling**, cannot handle conditions or loops \\ 
 \hline
 Automatic differentiation (AD) & Exact, fast & Complex to implement \\ 
 \hline
\end{tabular}
\end{center}

\subsection*{Automatic Differentiation}

Automatic differentiation (AD) is a set of techniques to evaluate the derivative of a function specified by a computer program. Unlike symbolic differentiation, which manipulates mathematical expressions to find the derivative, and numerical differentiation, which approximates the derivative using finite differences, AD computes the derivative using the actual operations performed in the function, ensuring exact results up to machine precision.

AD exploits the fact that any program, no matter how complex, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule of calculus repeatedly to these operations, AD computes the derivatives efficiently and accurately.



\section*{An Example}

Consider the following function of 3 variables:

$$
f(x)=\left(x_{1} x_{2} \sin x_{3}+e^{x_{1} x_{2}}\right) / x_{3}
$$

$$
\begin{aligned}
& x_{4}=x_{1} * x_{2} \\
& x_{5}=\sin x_{3} \\
& x_{6}=e^{x_{4}} \\
& x_{7}=x_{4} * x_{5} \\
& x_{8}=x_{6}+x_{7} \\
& x_{9}=x_{8} / x_{3}
\end{aligned}
$$



\section*{The Forward Mode}

\subsubsection*{An Overview}
Forward-mode autodiff is neither numerical differentiation nor symbolic differentiation, but in some ways, it is their love child. It relies on dual numbers, which are (weird but fascinating) numbers of the form \(a + b\epsilon\) where \(a\) and \(b\) are real numbers and \(\epsilon\) is an infinitesimal number such that \(\epsilon^2 = 0\) (but \(\epsilon \neq 0\)). You can think of the dual number \(42 + 24\epsilon\) as something akin to \(42.0000\ldots000024\) with an infinite number of 0s (but of course, this is simplified just to give you some idea of what dual numbers are). A dual number is represented in memory as a pair of floats. For example, \(42 + 24\epsilon\) is represented by the pair \((42.0, 24.0)\).

Dual numbers can be added, multiplied, and so on, as shown in the equations:
\[
\lambda (a + b\epsilon) = \lambda a + \lambda b\epsilon
\]
\[
(a + b\epsilon) + (c + d\epsilon) = (a + c) + (b + d)\epsilon
\]
\[
(a + b\epsilon) \times (c + d\epsilon) = ac + (ad + bc)\epsilon + bd\epsilon^2 = ac + (ad + bc)\epsilon
\]

Most importantly, it can be shown that \(h(a + b\epsilon) = h(a) + b \times h'(a)\epsilon\), so computing \(h(a + \epsilon)\) gives you both \(h(a)\) and the derivative \(h'(a)\) in just one shot. The computation of the partial derivative of \(f(x, y)\) with regards to \(x\) at \(x = 3\) and \(y = 4\) involves computing \(f(3 + \epsilon, 4)\); this will output a dual number whose first component is equal to \(f(3, 4)\) and whose second component is equal to \(\frac{\partial f}{\partial x}(3, 4)\).

To compute \(\frac{\partial f}{\partial y}(3, 4)\) we would have to go through the graph again, but this time with \(x = 3\) and \(y = 4 + \epsilon\).

So forward-mode autodiff is much more accurate than numerical differentiation, but it suffers from the same major flaw: if there were 1,000 parameters, it would require 1,000 passes through the graph to compute all the partial derivatives. This is where reverse-mode autodiff shines: it can compute all of them in just two passes through the graph.


\subsubsection*{More in detail}

In the forward mode of automatic differentiation, we evaluate and carry forward a directional derivative of each intermediate variable $x_{i}$ in a given direction $p \in \mathbb{R}^{n}$, simultaneously with the evaluation of $x_{i}$ itself. For the three-variable example above, we use the following notation for the directional derivative for $p$ associated with each variable:

$$
D_{p} x_{i} \stackrel{\text { def }}{=}\left(\nabla x_{i}\right)^{T} p=\sum_{j=1}^{3} \frac{\partial x_{i}}{\partial x_{j}} p_{j}, \quad i=1,2, \ldots, 9
$$

where $\nabla$ indicates the gradient with respect to the three independent variables. Our goal is to evaluate $D_{p} x_{9}$, which is the same as the directional derivative $\nabla f(x)^{T} p$. We note immediately that initial values $D_{p} x_{i}$ for the independent variables $x_{i}, i=1,2,3$, are simply the components $p_{1}, p_{2}, p_{3}$ of $p$. The direction $p$ is referred to as the seed vector.\\ \\
As soon as the value of $x_{i}$ at any node is known, we can find the corresponding value of $D_{p} x_{i}$ from the chain rule. For instance, suppose we know the values of $x_{4}, D_{p} x_{4}, x_{5}$, and $D_{p} x_{5}$, and we are about to calculate $x_{7}$. We have that $x_{7}=x_{4} x_{5}$; that is, $x_{7}$ is a function of the two variables $x_{4}$ and $x_{5}$, which in turn are functions of $x_{1}, x_{2}, x_{3}$. By applying the chain rule, we have that:
$$
\nabla x_{7}=\frac{\partial x_{7}}{\partial x_{4}} \nabla x_{4}+\frac{\partial x_{7}}{\partial x_{5}} \nabla x_{5}=x_{5} \nabla x_{4}+x_{4} \nabla x_{5}
$$

By taking the inner product of both sides of this expression with $p$ and applyingthe chain rule in the multivariate context we obtain:

$$
D_{p} x_{7}=\frac{\partial x_{7}}{\partial x_{4}} D_{p} x_{4}+\frac{\partial x_{7}}{\partial x_{5}} D_{p} x_{5}=x_{5} D_{p} x_{4}+x_{4} D_{p} x_{5}
$$\\
The directional derivatives $D_{p} x_{i}$ are therefore evaluated side by side with the intermediate results $x_{i}$, and at the end of the process we obtain $D_{p} x_{9}=D_{p} f=\nabla f(x)^{T} p$.\\ \\
The principle of the forward mode is straightforward enough, but what of its practical implementation and computational requirements? First, we repeat that the user does not need to construct the computational graph, break the computation down into elementary operations, or identify intermediate variables. The automatic differentiation software should perform these tasks implicitly and automatically. Nor is it necessary to store
the information $x_{i}$ and $D_{p} x_{i}$ for every node of the computation graph at once (which is just as well, since this graph can be very large for complicated functions). Once all the children of any node have been evaluated, its associated values $x_{i}$ and $D_{p} x_{i}$ are not needed further and may be overwritten in storage.\\ \\
The key to practical implementation is the side-by-side evaluation of $x_{i}$ and $D_{p} x_{i}$. The automatic differentiation software associates a scalar $D_{p} w$ with any scalar $w$ that appears in the evaluation code. Whenever $w$ is used in an arithmetic computation, the software performs an associated operation (based on the chain rule) with the gradient vector $D_{p} w$. For instance, if $w$ is combined in a division operation with another value $y$ to produce a new value $z$, that is,
$$
z \leftarrow \frac{w}{y}
$$

we use $w, z, D_{p} w$, and $D_{p} y$ to evaluate the directional derivative $D_{p} z$ as follows:

$$
D_{p} z \leftarrow \frac{1}{y} D_{p} w-\frac{w}{y^{2}} D_{p} y
$$

To obtain the complete gradient vector, we can carry out this procedure simultaneously for the $n$ seed vectors $p=e_{1}, e_{2}, \ldots, e_{n}$. We see that $p=e_{j}$ implies that $D_{p} f=\partial f / \partial x_{j}, j=1,2, \ldots, n$. We notethat the additional cost of evaluating $f$ and $\nabla f$ (over the cost of evaluating $f$ alone) may be significant. It is difficult to obtain an exact bound on the increase in computation, since the costs of retrieving and storing the data should also be taken into account. The storage requirements may also increase by a factor as large as $n$, since we now have to store $n$ additional scalars $D_{e_{j}} x_{i}, j=1,2, \ldots, n$, alongside each intermediate variable $x_{i}$. 

\section*{The Reverse Mode}

\subsection*{An Overview}
Reverse-mode autodiff is the solution implemented by TensorFlow. It first goes through the graph in the forward direction (i.e., from the inputs to the output) to compute the value of each node. Then it does a second pass, this time in the reverse direction (i.e., from the output to the inputs) to compute all the partial derivatives. During the first pass, all the node values were computed, starting from \(x = 3\) and \(y = 4\). You can see those values at the bottom right of each node (e.g., \(x \times x = 9\)). The nodes are labeled \(n_1\) to \(n_7\) for clarity. The output node is \(n_7: f(3,4) = n_7 = 42\).

The idea is to gradually go down the graph, computing the partial derivative of \(f(x,y)\) with regards to each consecutive node, until we reach the variable nodes. For this, reverse-mode autodiff relies heavily on the chain rule, shown in the equations:
\[
\frac{\partial f}{\partial x} = \frac{\partial f}{\partial n_i} \times \frac{\partial n_i}{\partial x}
\]
Since \(n_7\) is the output node, \(f = n_7\) so trivially \(\frac{\partial f}{\partial n_7} = 1\).

Letâ€™s continue down the graph to \(n_5\): how much does \(f\) vary when \(n_5\) varies? The answer is:
\[
\frac{\partial f}{\partial n_5} = \frac{\partial f}{\partial n_7} \times \frac{\partial n_7}{\partial n_5}
\]
We already know that \(\frac{\partial f}{\partial n_7} = 1\), so all we need is \(\frac{\partial n_7}{\partial n_5}\). Since \(n_7\) simply performs the sum \(n_5 + n_6\), we find that:
\[
\frac{\partial n_7}{\partial n_5} = 1, \quad \frac{\partial f}{\partial n_5} = 1 \times 1 = 1
\]

Now we can proceed to node \(n_4\): how much does \(f\) vary when \(n_4\) varies? The answer is:
\[
\frac{\partial f}{\partial n_4} = \frac{\partial f}{\partial n_5} \times \frac{\partial n_5}{\partial n_4}
\]
Since \(n_5 = n_4 \times n_2\), we find that:
\[
\frac{\partial n_5}{\partial n_4} = n_2, \quad \frac{\partial f}{\partial n_4} = 1 \times n_2 = 4
\]

The process continues until we reach the bottom of the graph. At that point, we will have calculated all the partial derivatives of \(f(x,y)\) at the point \(x = 3\) and \(y = 4\). In this example, we find \(\frac{\partial f}{\partial x} = 24\) and \(\frac{\partial f}{\partial y} = 10\). Sounds about right!

Reverse-mode autodiff is a very powerful and accurate technique, especially when there are many inputs and few outputs, since it requires only one forward pass plus one reverse pass per output to compute all the partial derivatives for all outputs with regards to all the inputs. Most importantly, it can deal with functions defined by arbitrary code. It can also handle functions that are not entirely differentiable,


\subsection*{More in detail}

The reverse mode of automatic differentiation does not perform function and gradient evaluations concurrently. Instead, after the evaluation of $f$ is complete, it recovers the partial
derivatives of $f$ with respect to each variable $x_{i}$-independent and intermediate variables alike-by performing a reverse sweep of the computational graph. At the conclusion of this process, the gradient vector $\nabla f$ can be assembled from the partial derivatives $\partial f / \partial x_{i}$ with respect to the independent variables $x_{i}, i=1,2, \ldots, n$.

Instead of the gradient vectors $D_{p} x_{i}$ used in the forward mode, the reverse mode associates a scalar variable $\bar{x}_{i}$ with each node in the graph; information about the partial derivative $\partial f / \partial x_{i}$ is accumulated in $\bar{x}_{i}$ during the reverse sweep. The $\bar{x}_{i}$ are sometimes called the adjoint variables, and we initialize their values to zero, with the exception of the rightmost node in the graph (node $N$, say), for which we set $\bar{x}_{N}=1$. This choice makes sense because $x_{N}$ contains the final function value $f$, so we have $\partial f /$ $\partial x_{N}=1$.

The reverse sweep makes use of the following observation, which is again based on the chain rule: For any node $i$, the partial derivative $\partial f / \partial x_{i}$ can be built up from the partial derivatives $\partial f / \partial x_{j}$ corresponding to its child nodes $j$ according to the following formula:

$$
\frac{\partial f}{\partial x_{i}}=\sum_{j \text { a child of } i} \frac{\partial f}{\partial x_{j}} \frac{\partial x_{j}}{\partial x_{i}}
$$

For each node $i$, we add the right-hand-side term to $\bar{x}_{i}$ as soon as it becomes known; that is, we perform the operation

$$
\bar{x}_{i}+=\frac{\partial f}{\partial x_{j}} \frac{\partial x_{j}}{\partial x_{i}}
$$

(In this expression and the ones below, we use the arithmetic notation of the programming language $\mathrm{C}$, in which $x+=a$ means $x \leftarrow x+a$.) Once contributions have been received from all the child nodes of $i$, we have $\bar{x}_{i}=\partial f / \partial x_{i}$, so we declare node $i$ to be "finalized." At this point, node $i$ is ready to contribute a term to the summation for each of its parent nodes. The process continues in this fashion until all nodes are finalized. Note that for derivative evaluation, the flow of computation in the graph is from children to parents-the opposite direction to the computation flow for function evaluation.

During the reverse sweep, we work with numerical values, not with formulae or computer code involving the variables $x_{i}$ or the partial derivatives $\partial f / \partial x_{i}$. During the forward sweep-the evaluation of $f$-we not only calculate the values of each variable $x_{i}$, but we also calculate and store the numerical values of each partial derivative $\partial x_{j} / \partial x_{i}$. Each of these partial derivatives is associated with a particular arc of the computational graph. The numerical values of $\partial x_{j} / \partial x_{i}$ computed during the forward sweep are then used during the reverse sweep.




The main appeal of the reverse mode is that its computational complexity is low for the scalar functions $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$ discussed here. The extra arithmetic associated with the gradient computation is at most four or five times the arithmetic needed to evaluate the function alone. \\
As we noted above, the forward mode may require up to $n$ times more arithmetic to compute the gradient $\nabla f$ than to compute the function $f$ alone, making it appear uncompetitive with the reverse mode. When we consider vector functions $r: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$, the relative costs of the forward and reverse modes become more similar as $m$ increases, as we describe in the next section.\\
An apparent drawback of the reverse mode is the need to store the entire computational graph, which is needed for the reverse sweep. In principle, storage of this graph is not too difficult to implement. Whenever an elementary operation is performed, we can form and store a new node containing the intermediate result, pointers to the (one or two) parent nodes, and the partial derivatives associated with these arcs. During the reverse sweep, the nodes can be read in the reverse order to that in which they were written, giving a particularly simple access pattern. 
Unfortunately, the computational graph may require a huge amount of storage.



\subsection*{Matrix-Free Jacobian-Vector Product with Forward Mode Automatic Differentiation}

Forward mode automatic differentiation (AD) provides a very efficient and matrix-free way of computing Jacobian--vector products. This method utilizes the concept of a seed vector to guide the derivative computations. The seed vector \(\dot{\mathbf{x}}\) is initialized with \(\mathbf{r}\), defining the direction of differentiation right at the start:

\[
J_f \mathbf{r} = \begin{bmatrix}
    \frac{\partial y_1}{\partial x_1} & \cdots & \frac{\partial y_1}{\partial x_n} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial y_m}{\partial x_1} & \cdots & \frac{\partial y_m}{\partial x_n} 
\end{bmatrix} \begin{bmatrix}
    r_1 \\
    \vdots \\
    r_n
\end{bmatrix},
\]

This approach enables the computation of the Jacobian--vector product in just one forward pass of the AD process by efficiently propagating changes along the direction specified by the seed vector \(\mathbf{r}\). As a special case, when \( f : \mathbb{R}^n \to \mathbb{R} \), we can obtain the directional derivative along a given vector \(\mathbf{r}\) as:

\[
\nabla f \cdot \mathbf{r}
\]

This is achieved by starting the AD computation with the values \(\dot{\mathbf{x}} = \mathbf{r}\), directly obtaining the rate of change of \(f\) in the direction of \(\mathbf{r}\). 


Forward mode AD is efficient and straightforward for functions $f : \mathbb{R} \to \mathbb{R}^m$, as all the derivatives $\frac{dy_i}{dx}$ can be computed with just one forward pass. Conversely, in the other extreme of $f : \mathbb{R}^n \to \mathbb{R}$, forward mode AD requires $n$ evaluations to compute the gradient
\[
\nabla f = \left(\frac{\partial y}{\partial x_1}, \dots, \frac{\partial y}{\partial x_n}\right),
\]
which also corresponds to a $1 \times n$ Jacobian matrix that is built one column at a time with the forward mode in $n$ evaluations.



\subsection*{Dual-numbers}
They are similar to complex numbers. 
\[
    a + b\epsilon \hspace{0.3cm} \text{ with }\hspace{0.3cm} a,b \in \mathbb{R} \hspace{0.3cm} \text{and} \hspace{0.3cm} \epsilon \neq 0, \epsilon^2 = 0    
\]
Where $a$ is the real part and $b$ is the dual part. Basic operations with dual numbers:
\[
    (a + b\epsilon) + (c + d\epsilon) = (a + c) + (b + d)\epsilon    
\]
\[
    (a + b\epsilon)(c + d\epsilon) = ac + (ad + bc)\epsilon     
\]
Why this algebra is useful to our purposes? Consider the generic function $f$, we want to evaluate it with a dual number:
\[
    f(a + b\epsilon) = f(a) + f'(a)b\epsilon + \underbrace{\dfrac{f''(a)}{2}b^2\epsilon^2   + \dots}_{0}
\]
We have used the Taylor expansion, the last terms are zero because of $\epsilon^2$. Notice that, if we have unitary dual part ($b = 1$), we obtain the derivative of $f$ in $a$. This is the key point of the dual numbers:
\[
    b=1 \implies a + b\epsilon = a + \epsilon \implies f(a + \epsilon) = f(a) + f'(a)\epsilon    
\]
The last term is again a dual number in which the real part is the value of the function in $a$ and the dual part is the derivative of the function in $a$.\\

Let's check if the derivative properties are actually respected, suppose to have $f(x) = g(x)h(x)$:
\[
    \begin{split}
        f(a + \epsilon) &= g(a + \epsilon)h(a + \epsilon)\\
        &= [g(a) + g'(a)\epsilon][h(a) + h'(a)\epsilon]\\
        &= g(a)h(a) + g(a)h'(a)\epsilon + g'(a)h(a)\epsilon + g'(a)h'(a)\epsilon^2\\
        &= g(a)h(a) + \underbrace{[g(a)h'(a) + g'(a)h(a)]}_{\text{der. of the product}}\epsilon\\
    \end{split}
\]
Or $f(x) = g(h(x))$:
\[
    \begin{split}
        f(a + \epsilon) &= g(h(a + \epsilon))\\
        &= g(h(a) + h'(a)\epsilon)\\
        &= \underbrace{ g(h(a))}_{f(a)} + \underbrace{g'(h(a))h'(a)\epsilon}_{f'(a)\epsilon}\\        
    \end{split}
\]


\subsection*{Usage of Automatic Differentiation}

Consider a function \( f: \mathbb{R}^n \to \mathbb{R} \). Automatic Differentiation (AD) can efficiently compute the Jacobian matrix \( \mathbf{J} \) using either Forward Mode (FM) or Backward Mode (BM). In FM, the operation count is approximately \( n \times c \times \operatorname{ops}(f) \), where \( c \approx 2.5 \). In BM, it is \( m \times c \times \operatorname{ops}(f) \), with \( m \) and \( n \) representing the dimensions of the output and input spaces, respectively. FM computes \( \mathbf{J}\underline{r} \) by setting \( \underline{\dot{x}} = \underline{r} \), which represents the product of the Jacobian with a vector. Conversely, BM computes \( \mathbf{J}^\top \underline{r} \) by setting \( \bar{\underline{y}} = \underline{r} \), which corresponds to the transpose Jacobian-vector product. BM necessitates storing all intermediate values from the forward computation for use during the backward computation.

We can also devise schemes based on the reverse mode for calculating Hessian-vector products \( \nabla^2 f(x)q \), or the full Hessian \( \nabla^2 f(x) \). A scheme for obtaining \( \nabla^2 f(x)q \) proceeds as follows. We start by using the forward mode to evaluate both \( f \) and \( \nabla f(x)^T q \), by accumulating the two variables \( x_i \) and \( D_q x_i \) during the forward sweep in the manner described above. We then apply the reverse mode in the normal fashion to the computed function \( \nabla f(x)^T q \). At the end of the reverse sweep, the nodes \( i = 1, 2, \ldots, n \) of the computational graph that correspond to the independent variables will contain
\[
\frac{\partial}{\partial x_i} (\nabla f(x)^T q) = [\nabla^2 f(x)q]_i, \quad i = 1, 2, \ldots, n.
\]


The number of arithmetic operations required to obtain \( \nabla^2 f(x)q \) by this procedure increases by only a modest factor, independent of \( n \), over the evaluation of \( f \) alone. By the usual analysis for the forward mode, we see that the computation of \( f \) and \( \nabla f(x)^T q \) jointly requires a small multiple of the operation count for \( f \) alone, while the reverse sweep introduces a further factor of at most 5. The total increase factor is approximately 12 over the operation count for \( f \) alone.

For more complex requirements, such as when the full Hessian \( \nabla^2 f(x) \) is needed or when \( \nabla^2 f(x) q \) must be computed, a combination of FM and BM is employed. The forward mode evaluates both \( f \) and \( \nabla f(x)^T q \), and the reverse mode subsequently computes \( \nabla^2 f(x) q \). This procedure increases the operation count by a factor of approximately 12 over the evaluation of \( f \) alone. Utilizing different seed vectors \( q \) (e.g., \( e_1, e_2, \ldots, e_n \) for the full Hessian) and possibly employing graph-coloring techniques when the Hessian is sparse, can significantly reduce the computational cost compared to direct evaluation methods.